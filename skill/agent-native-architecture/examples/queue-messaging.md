# Queue-Based Agent Messaging Patterns

Cloudflare Queues enable asynchronous agent-to-agent communication, workflow orchestration, and reliable message delivery with automatic retries.

## Core Pattern: Agent-to-Agent Messaging

```typescript
// Agent sends messages to other agents via Queue
async function sendToAgent(
  env: Env,
  recipientId: string,
  message: AgentMessage
): Promise<void> {
  await env.AGENT_QUEUE.send({
    recipientId,
    senderId: 'current-agent',
    message,
    timestamp: Date.now(),
  });
}

// Queue consumer routes messages to recipient agents
export default {
  async queue(batch: MessageBatch<AgentMessage>, env: Env): Promise<void> {
    for (const message of batch.messages) {
      // Get recipient agent's Durable Object
      const agentId = env.AGENT_STATE.idFromName(message.body.recipientId);
      const agent = env.AGENT_STATE.get(agentId);

      // Deliver message to agent
      await agent.fetch(
        new Request('https://agent/message', {
          method: 'POST',
          body: JSON.stringify(message.body),
        })
      );

      message.ack();
    }
  }
};
```

## Multi-Step Workflow via Queues

```typescript
// Agent orchestrates workflow by queuing steps
export class WorkflowOrchestrator extends DurableObject {
  async startWorkflow(workflowData: any): Promise<void> {
    // Agent decides workflow steps
    const plan = await runAgent({
      systemPrompt: `
You orchestrate multi-step workflows via Queue messaging.

For this workflow, decide:
1. What steps are needed
2. What order they should execute
3. What data each step needs
4. How to handle dependencies between steps

Queue each step as a message. Steps will execute asynchronously.
      `,
      workflowData,
      tools: [
        tool("queue_step", {
          step: z.string(),
          data: z.any(),
          dependencies: z.array(z.string()).optional(),
        }),
      ]
    });

    // Queue workflow steps
    for (const step of plan.steps) {
      await this.env.WORKFLOW_QUEUE.send({
        workflowId: workflowData.id,
        step: step.name,
        data: step.data,
        dependencies: step.dependencies,
      });
    }

    // Store workflow state
    await this.ctx.storage.put(`workflow:${workflowData.id}`, {
      status: 'running',
      steps: plan.steps,
      startedAt: Date.now(),
    });
  }
}

// Workflow step processor
export default {
  async queue(batch: MessageBatch<WorkflowStep>, env: Env): Promise<void> {
    for (const message of batch.messages) {
      const step = message.body;

      // Check if dependencies are complete
      const ready = await this.checkDependencies(step, env);
      if (!ready) {
        message.retry({ delaySeconds: 5 }); // Wait for dependencies
        continue;
      }

      // Agent processes the step
      const result = await runAgent({
        systemPrompt: `
You are a workflow step processor.

Step: ${step.step}
Data: ${JSON.stringify(step.data)}

Execute this step and decide:
1. Was it successful?
2. What should happen next?
3. Should we queue additional steps?
4. Update workflow state?
        `,
        step,
        tools: [
          tool("complete_step", { result: z.any() }),
          tool("queue_next_step", { step: z.string(), data: z.any() }),
          tool("fail_workflow", { reason: z.string() }),
        ]
      });

      // Update workflow state
      await this.updateWorkflowState(step.workflowId, step.step, result, env);

      message.ack();
    }
  }
};
```

## Pub/Sub Pattern with Queues

```typescript
// Publisher agent broadcasts events
export class PublisherAgent extends DurableObject {
  async publishEvent(event: AgentEvent): Promise<void> {
    // Agent decides who should receive this event
    const distribution = await runAgent({
      systemPrompt: `
You publish events to interested agents.

Event: ${event.type}
Data: ${JSON.stringify(event.data)}

Subscribers:
- analyzer-agents: Interested in data_received events
- alert-agents: Interested in threshold_exceeded events
- audit-agents: Interested in all events

Decide which agents should receive this event.
      `,
      event,
      tools: [
        tool("send_to_subscriber", {
          subscriberId: z.string(),
          event: z.any(),
        }),
      ]
    });

    // Broadcast to subscribers via Queue
    for (const subscriber of distribution.subscribers) {
      await this.env.EVENT_QUEUE.send({
        subscriberId: subscriber,
        event,
        publishedAt: Date.now(),
      });
    }
  }
}

// Subscriber agents consume events
export class SubscriberAgent extends DurableObject {
  async handleEvent(event: AgentEvent): Promise<void> {
    // Agent processes subscribed event
    await runAgent({
      systemPrompt: `
You receive events you're subscribed to.

Process this event and decide:
- Should you take action?
- Update your state?
- Publish derived events?
      `,
      event,
    });
  }
}
```

## Priority Queue Pattern

```typescript
// Agent assigns priority to messages
export class PriorityAgent extends DurableObject {
  async processTask(task: Task): Promise<void> {
    // Agent decides task priority
    const analysis = await runAgent({
      systemPrompt: `
Analyze this task and assign priority level.

Priority levels:
- critical: Process immediately (separate queue)
- high: Process within 1 minute
- normal: Process within 5 minutes
- low: Process when capacity available

Consider:
- User impact
- Deadline sensitivity
- Resource requirements
- Dependencies
      `,
      task,
    });

    // Queue to appropriate priority queue
    switch (analysis.priority) {
      case 'critical':
        await this.env.CRITICAL_QUEUE.send(task);
        break;
      case 'high':
        await this.env.HIGH_PRIORITY_QUEUE.send(task);
        break;
      case 'normal':
        await this.env.NORMAL_QUEUE.send(task);
        break;
      case 'low':
        await this.env.LOW_PRIORITY_QUEUE.send(task);
        break;
    }
  }
}

// Priority-aware queue consumers
export default {
  async queue(batch: MessageBatch<Task>, env: Env): Promise<void> {
    // Process based on queue type
    const queueType = env.QUEUE_TYPE; // 'critical', 'high', 'normal', 'low'

    for (const message of batch.messages) {
      // Agent processes with priority context
      await runAgent({
        systemPrompt: `
You are processing a ${queueType} priority task.

${queueType === 'critical' ? 'This is urgent. Handle immediately.' : ''}
${queueType === 'low' ? 'Process efficiently but thoroughness matters more than speed.' : ''}

Task: ${JSON.stringify(message.body)}
        `,
        task: message.body,
      });

      message.ack();
    }
  }
};
```

## Error Handling and Dead Letter Queue

```typescript
// Agent decides error handling strategy
export default {
  async queue(batch: MessageBatch<AgentTask>, env: Env): Promise<void> {
    for (const message of batch.messages) {
      try {
        await this.processMessage(message, env);
        message.ack();

      } catch (error) {
        // Agent analyzes error and decides what to do
        const decision = await this.agentHandleError(error, message, env);

        switch (decision.action) {
          case 'retry':
            // Retry with backoff
            message.retry({
              delaySeconds: decision.delaySeconds,
            });
            break;

          case 'dlq':
            // Send to dead letter queue for manual review
            await env.DEAD_LETTER_QUEUE.send({
              originalMessage: message.body,
              error: error.message,
              attempts: message.attempts,
              timestamp: Date.now(),
            });
            message.ack(); // Remove from main queue
            break;

          case 'ignore':
            // Agent decided to ignore this error
            console.log('Agent decided to ignore error:', error);
            message.ack();
            break;
        }
      }
    }
  },

  async agentHandleError(
    error: Error,
    message: QueueMessage,
    env: Env
  ): Promise<ErrorDecision> {
    return runAgent({
      systemPrompt: `
A message processing error occurred.

Error: ${error.name} - ${error.message}
Attempt: ${message.attempts}
Message age: ${Date.now() - message.timestamp}ms

Decide how to handle this:

RETRY if:
- Network timeout (transient)
- Rate limit (wait for backoff)
- Temporary service unavailable
- Attempts < 3

DEAD LETTER QUEUE if:
- Invalid message format (permanent)
- Authorization failed (won't succeed on retry)
- Max attempts exceeded
- Critical error requiring investigation

IGNORE if:
- Duplicate processing detected
- Message no longer relevant
- Acceptable failure scenario

Return: { action: 'retry' | 'dlq' | 'ignore', delaySeconds?: number, reason: string }
      `,
      error: {
        name: error.name,
        message: error.message,
        stack: error.stack,
      },
      message: {
        attempts: message.attempts,
        timestamp: message.timestamp,
        body: message.body,
      },
    });
  }
};

// Dead letter queue processor (manual intervention agent)
export default {
  async queue(batch: MessageBatch<DeadLetter>, env: Env): Promise<void> {
    // Agent reviews failed messages
    for (const message of batch.messages) {
      const review = await runAgent({
        systemPrompt: `
Review this failed message from the dead letter queue.

Decide:
1. What went wrong?
2. Can it be recovered?
3. Should we re-queue it?
4. Log for investigation?
5. Notify someone?
        `,
        deadLetter: message.body,
      });

      if (review.shouldRequeue) {
        // Agent decided to retry
        await env.MAIN_QUEUE.send(message.body.originalMessage);
      }

      if (review.shouldNotify) {
        // Agent decided human intervention needed
        await this.notifyHumans(message.body, review.reason, env);
      }

      message.ack();
    }
  }
};
```

## Batch Processing Pattern

```typescript
// Agent batches messages for efficiency
export class BatchProcessor extends DurableObject {
  private pendingBatch: any[] = [];
  private batchTimer: number | null = null;

  async queueItem(item: any): Promise<void> {
    this.pendingBatch.push(item);

    // Agent decides when to process batch
    const decision = await runAgent({
      systemPrompt: `
You manage batched message processing.

Current batch size: ${this.pendingBatch.length}
Time since first item: ${this.getTimeSinceFirstItem()}ms

Decide if you should:
- Process batch now (good size or time elapsed)
- Wait for more items (batch too small, under time limit)

Consider:
- Batch efficiency vs latency trade-off
- Size: optimal around 50-100 items
- Time: max wait 30 seconds
      `,
      batchSize: this.pendingBatch.length,
      timeSinceFirst: this.getTimeSinceFirstItem(),
    });

    if (decision.shouldProcess) {
      await this.processBatch();
    } else {
      // Set timer for max wait
      if (!this.batchTimer) {
        this.batchTimer = setTimeout(() => this.processBatch(), 30000);
      }
    }
  }

  async processBatch(): Promise<void> {
    if (this.pendingBatch.length === 0) return;

    // Send batch as single queue message
    await this.env.BATCH_QUEUE.send({
      items: this.pendingBatch,
      count: this.pendingBatch.length,
      timestamp: Date.now(),
    });

    // Reset batch
    this.pendingBatch = [];
    if (this.batchTimer) {
      clearTimeout(this.batchTimer);
      this.batchTimer = null;
    }
  }
}
```

## Rate Limiting with Queues

```typescript
// Agent respects rate limits via queue pacing
export default {
  async queue(batch: MessageBatch<ApiCall>, env: Env): Promise<void> {
    // Get current rate limit state
    const rateLimitState = await this.getRateLimitState(env);

    for (const message of batch.messages) {
      // Agent decides if we can proceed
      const decision = await runAgent({
        systemPrompt: `
You manage API calls with rate limit awareness.

Current state:
- Calls this minute: ${rateLimitState.callsThisMinute}
- Limit: ${rateLimitState.limit} per minute
- Reset in: ${rateLimitState.resetIn}ms

Should you:
- Process this call now
- Retry with delay
- Queue to next time window
        `,
        rateLimitState,
        message: message.body,
      });

      if (decision.proceed) {
        await this.makeApiCall(message.body, env);
        await this.incrementRateLimit(env);
        message.ack();
      } else {
        // Retry when rate limit resets
        message.retry({
          delaySeconds: Math.ceil(decision.waitMs / 1000),
        });
      }
    }
  }
};
```

## Key Principles

1. **Async by default** - Queues decouple agents, enable parallel processing
2. **Reliable delivery** - Messages persist until acknowledged
3. **Automatic retries** - Built-in exponential backoff
4. **Agent decides routing** - Don't hardcode message flow
5. **Priority via separate queues** - Different queues for different urgency
6. **Batch for efficiency** - Agent decides optimal batch size/timing
7. **DLQ for failures** - Separate queue for messages requiring intervention

## Anti-Patterns to Avoid

**Don't encode workflow in code**
```typescript
// Wrong - you hardcode the workflow
async function processOrder(order) {
  await step1(order);
  await step2(order);
  await step3(order);
}

// Right - agent decides workflow via queue messages
await agent.planWorkflow(order); // Agent queues steps
```

**Don't ignore retry strategy**
```typescript
// Wrong - always retry
catch (error) {
  message.retry();
}

// Right - agent decides
const decision = await agent.analyzeError(error);
if (decision.shouldRetry) message.retry();
```

**Don't couple agents tightly**
```typescript
// Wrong - direct coupling
const result = await otherAgent.process(data);

// Right - queue-based communication
await env.QUEUE.send({ recipientId: 'other-agent', data });
```